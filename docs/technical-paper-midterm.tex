\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Obstacle Tower Challenge}

\author{\IEEEauthorblockN{Aditya Chandupatla}
\IEEEauthorblockA{\textit{University of Southern California}\\
Los Angeles, CA \\
chandupa@usc.edu}
\and
\IEEEauthorblockN{Avantika Shenoy}
\IEEEauthorblockA{\textit{University of Southern California}\\
Los Angeles, CA \\
avantika@usc.edu}
\and
\IEEEauthorblockN{Hardik Surana}
\IEEEauthorblockA{\textit{University of Southern California}\\
Los Angeles, CA \\
hsurana@usc.edu}
\and
\IEEEauthorblockN{Pranshu Dave}
\IEEEauthorblockA{\textit{University of Southern California}\\
Los Angeles, CA \\
pranshur@usc.edu}
}

\maketitle

\begin{abstract}
This paper talks about the "Obstacle Tower Challenge" game. It is an environment which is generated by a specific process, sometimes referred to as "procedurally generated". The learning agent has to play this game which is composed many floors.

To tackle this challenge, the following problems must be solved: the vision system for the agent, the three dimensional spatial movement-based skills of the agent, planning on an abstract level, and generalizing the decision-making process. As the agent progresses, the difficulty of the game keeps on increasing.

The way the agent progresses onto the next level is by identifying a set of stairs which will enable to agent to go to the next level. To make it more complicated, we have multiple rooms with a plethora of challenges. Some of these challenges are computer vision based, few are related to decision-making ability, and the rest are related to figuring out how to traverse the floor. It therefore imperative that our agent learns how to generalise and make optimal decisions in such a way that it solves all the aforementioned tasks.
\end{abstract}

\begin{IEEEkeywords}
Reinforcement learning, Asynchronous Advantage Actor-Critic (A3C), Proximal Policy Optimization (PPO), Trust Region Policy Optimization (TRPO), Actor-Critic with Experience Replay (ACER), Importance Weighted Actor Learner Architecture (IMPALA)
\end{IEEEkeywords}

\section{Introduction}
Regardless of the improvements and progress made in instrumenting new algorithms, most scientists are still focusing on arcane methods of playing games such as Breakout and other similar games. The issue with these games is that the graphics involved are pretty rudimentary, and easy to predict. The algorithms might as well overfit to the game and mark it as solved. Therefore they are uninteresting and not fruitful.

Given these drawbacks, Unity developed a procedurally generated environment called the Obstacle Tower Challenge that would serve as an interesting tool which can be used as a yardstick to measure the progress made in contemporary artificial intelligence algorithms in reinforcement learning to their limits. Currently, the focus is on the agents computer vision capabilities, locomotion skills, abstract thinking, and the way they generalize. 
Better agents means better non-player-characters, thorough testing, and finally more engaging player experiences.

More recently, with advances in the field of reinforcement learning, novel game-based artificially intelligent agents have become possible. The agents learn to act based on the raw pixel input provided to them. Furthermore, with the advances in the field of deep learning, which is a field that excels at dealing with complex high-dimensional input, such as screen images, and also because of the advent of hardware accelerators such as GPU's (Graphical Processing Units), TPU's (Tensor Processing Units), etc, training a reinforcement learning agent has become commonplace.

Nevertheless, despite the hardware improvement and introduction of new reinforcement learning methods, targeting abstract-level gameplay is crucial if the world ever wants to make progress in the field of artificial general intelligence.

The primary purpose for which Unity has developed Obstacle tower challenge game was to show to the world that the way we progress on artificial general intelligence is by solving major broad areas in the field of artificial intelligence such as: computer vision, remembering state based information, forgetting information which is not desired, compressing high quality information, be it images or maps, into a small format, and also on how to navigate in a complex ever-evolving environment.

\textbf{Complex high-dimensional textured input}: Specifically, in this game, you'll see three dimensional rendering and near-real-time optical effects of the environment. This is accompanied by more fine-grained and textured environment compared to the games which were invented before this one.

\textbf{Procedurally generated floors and rooms}: As stated earlier, it is important that our agent doesn't make any suboptimal or greedy decisions because it not only has a short-term objective, but also a long-term objective.

\textbf{Physics}: Furthermore, to make the game more realistic, the simulation strongly relies on fundamental laws of physics. 

\textbf{Procedurally generated visuals}: Not only is the game complex in terms of the way the map is organised, but as the agent progresses from one floor to another, the environment also changes to reflect the difficulty.

\section{Related Works}

The Obstacle Tower game has been introduced as a challenge to the community of AI developers by Unity. Here, we list the top three winners of that challenge which Unity has released:
\begin{itemize}
\item The first approach is by Alex. The way Alex approached the problem is rather intriguing. This is because, he first developed a convolutional neural network to make the agent understand how it is perceiving the environment. Later, behavioral cloning has been used to train the agent. This is unique technique wherein the agent learns from human examples. Lastly, PPO was used which he calls "Prierarchy" to fine tune the agent. The replacement has been done via KL-divergence in the PPO. The entire process is well-thought and therefore provided impressive results. However, the reason it is not truly successful is because it couldn't generalise the way it was initially expected to be.
\item Gianni and Miha are the other team which have done this. Their deliverable was: PPO with reduced action set and reward function. The approach was more-or-less similar to the one take by Alex, however, the way it started to differ is on later levels. The team did not have enough resources such as compute power, and training data set to make significant progress.
\item Songbin is the next player. They used a library called ML-Agents toolkit which has PPO algorithms. GRU also has been used in this model to ensure that the model remembers the sequence of actions it has taken. This is important to ensure generalisation because the agent has to not only know what was the previous action, but it must also know what set of actions it took for the past several timesteps. Experience replay buffer technique was also incorporated. Songbin has become an expert player in the entire pipeline of developing an agent, but the training process still lacked dataset and needed even more human simulation.
\end{itemize}

\textbf{Winner’s Approach}: The winner made use of two agents. One that solved level 1-9 and the other solved level 10 onwards. The agent that solved level 10 and above was trained with floors from 10-15. The sokoban puzzle problem was immediately solved as a result of this since the agent did not have to spend time solving the first ten floors and could focus on the puzzle alone.

To make it easy to play the game as a human, a reduced action space was introduced.  The CNN architecture from IMPALA was used since it is known to have performed better in terms of generalization. By rescaling a standard initialization the Fixup initialization solves the problem of vanishing gradient. Residual networks were also trained with Fixup and were found to be equally stable. Furthermore, with proper regularization, Fixup enables residual networks without normalization to achieve state-of-the-art performance in image classification and machine translation. The state classifier was trained with fewer labeled examples by using MixMatch.
Data augmentation was used in two scenarios by Alex: for behavior cloning by using traditional image data and while prierarchy training. With the use of mirroring data augmentation, which essentially means images and actions were imitated together, the quantity of training levels were doubled as each level had a mirror image associated with it. To resolve the problem of overfitting, data augmentation was used on the environment while training the agent with prierarchy. 


\section{Environment}
Obstacle Tower provides recognizable and configurable observation spaces, action spaces, and reward functions. The environment itself relies heavily on procedural generation at multiple levels of interaction. The Obstacle Tower environment uses the Unity platform and ML-Agents Toolkit [Juliani et al., 2018]. It can run on the Mac, Windows, and Linux platforms, and can be controlled via the OpenAI Gym interface for easy integration with existing DeepRL training frameworks [Brockman et al., 2016].

\begin{itemize}
	\item \textbf{Episode Dynamics}
	\item \textbf{Observation Space}
	\item \textbf{Action Space}
	\item \textbf{Reward Function}
\end{itemize}

\section{Methods (Software and Algorithms)}

\begin{figure*}
  \includegraphics[width=\textwidth,height=12cm]{6.png}
\end{figure*}

\subsection{\textbf{Asynchronous Methods for Deep Reinforcement Learning (Actor Critic Method)}}

Instead of experience replay, we asynchronously execute multiple agents in parallel, on multiple instances of the environment. This parallelism also removes the correlation of the agents’ data into a more stationary process, since at any given time-step the parallel agents will be experiencing a variety of different states. This simple idea enables a much larger spectrum of fundamental on-policy RL algorithms, such as Sarsa, n-step methods, and actor-critic methods, as well as off-policy RL algorithms such as Q-learning, to be applied robustly and effectively using deep neural networks.

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm]{7.png}
\end{figure}

\subsection{\textbf{Policy and Value Network Architectures}}

Network architectures for value and policy are equally important for the performance of our agent. Reinforcement learning implementations use two approaches to implement these networks. One is where there are separate networks for policy and value. In such an implementation, no parameters are shared between the two networks. The second uses a single network shared for both policy and value. The idea behind using a single network is that shared parameters allow the agent to learn both policy and values faster. 

We use the latter implementation for the purpose of this project. We have two different architectures for this purpose. A CNN based architecture to capture spatial features from agent’s visual observations. It has 3 convolutional layers and dense layers for predicting policy actions and value. The second model uses a CNN-GRU based architecture to capture both spatial and temporal dependencies. These architectures are detailed in the figures below. 

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm]{11.png}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm]{12.png}
\end{figure}

\subsection{\textbf{Implementation Tactics}}

In addition to developing model networks and architectures, we have also tried to use strategies or tactics to tweak the models to train better and faster.

\begin{itemize}
    \item \textbf{Action Space Reduction}: To reduce the time and space complexities of reinforcement learning algorithms, it is important to handle the action spaces associated with them. Large action spaces lead to large branching factors which leads to poor decision taking abilities in the agent. Fewer actions led to smaller branching factor leading to faster actions and better training decisions. In our implementation, we reduced the action space from 54 to 7. Our approach to choosing actions was to choose only those action combinations that are natural for a human to perform. In addition to this, we also fixed the floor generation seed and set the “visual-theme” parameter to 0. This makes sure that only the default theme is in place thereby removing generalization. 
    \item \textbf{Retro Mode}: Our game comes with a mode called “Retro”. In this mode, the supplemental information like number of keys in possession, time remaining and current floor are embedded inside the image. By setting the retro mode to False, the code was refactored to make sure the image is no longer embedded with the data and we receive the data separately as a tuple. This removes the issue of having to use computer vision and OCR to extract the data.
    \item \textbf{Updated Rewards}: Extrinsic rewards were updated to prioritize collection of keys, time orbs and crossing of floors. This trains the agent to focus on these tasks and penalizes it when it performs anything else.
\end{itemize}

\subsection{\textbf{Proximal Policy Optimization (PPO)}}

The heart of PPO is its loss function. The primary field of loss function consists is called a clipped surrogate objective. This consists of two parts:

Default objective for policy gradient: It pushes the objective functions towards regions with high positive advantage over baseline. It consists of two terms.

R-theta: This is the probability ratio of the current policy and old policy. 

Advantage: This estimates the relative value of selected action. 

Advantage objective can be both positive and negative and this changes the use of min operator:
	
If A > 0, our actions yielded better than expected return. So after A exceeds a point, the action has become more probable after the last gradient step, we don’t want to keep updating too much or else it might get worse. Hence clipping works in this case, it limits the effect of gradient update.

If A < 0, out actions yielded worse than expected return. If A < 0, action might become less probable, don’t keep reducing it’s likelihood too much now. Hence clipping works here as well.

The key thing to remember here is that the advantage function is noisy, so we don’t want to destroy a policy just based on a single estimate.

Clip: This part consists of a clipped version of the normal policy gradient objective function that constrains the value of policy loss between (1 + clipping threshold, 1 - clipping threshold) * advantage. It is responsible for discouraging the policy to deviate too much if the algorithm is already converging towards a currently found good policy.

\textbf{Why PPO?}\\

\begin{itemize}
\item Unstable Policy Update: In Many Policy Gradient Methods, policy updates are unstable because of larger step size, which leads to bad policy updates and when this new bad policy is used for learning then it leads to even worse policy. And if steps are small then it leads to slower learning. \\
\item Data Inefficiency: Many learning methods learn from current experience and discard the experiences after gradient updates. This makes the learning process slow as a neural net takes lots of data to learn.\\
\end{itemize}

PPO comes in handy to overcome the above issues.\\

In the next section, we will be presenting our results and a detailed analysis of what we have achieved by implementing A3C and PPO on the Obstacle Tower Challenge.

\section{Results and Analysis}

We have successfully trained 2 models so far using the A3C and PPO models. Each of the models have been trained for 100 game episodes totalling ~70,000 timesteps.

\begin{figure*}
  \includegraphics[width=\textwidth,height=10cm]{8.png}
\end{figure*}

Looking at the loss graphs we learn that since these algorithms are on-policy, they need a huge amount of training to develop a meaningful policy. Although the training done so far is fairly limited, we see a steady improvement in the collected rewards. Spikes in the loss function mean that the algorithm is exploring new trajectories which is consistent with expected results of initial training for the models. 

In the graphs for the A3C models below, we see a maximum reward of 0.6 which is reached very early in the training period with a gradual degradation later on. 

\begin{figure}[htp]
    \centering
    \includegraphics[width=10cm]{1.png}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm]{2.png}
\end{figure}

The PPO model performs better in terms of registering lower loss values and showing fewer loss spikes. But, the overall score achieved is lesser.

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm]{3.png}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm]{4.png}
\end{figure}

An interesting insight from the distributed training using thread-based parallelism shows that although we get a significant performance improvement from using multiple workers, as seen in the below graph, we reach optimal performance using 4 workers. This is against the idea that more workers means faster training. A hypothesis behind a decrease after this may be due to the increased inter-thread communication cost that happens during synchronization of weights with the master/global agent network. To put this in perspective, all the workers were created in threads on the same local machine. We expect to achieve better performance with distributed tensorflow which will enable us to employ more than 1 machine.

\begin{figure}[htp]
    \centering
    \includegraphics[width=7cm]{5.png}
\end{figure}

\section{Limitations}

Actor-critic methods combine policy gradient methods with a learned value function. With DQN, we only had the learned value function — the Q-function — and the “policy” we followed was simply taking the action that maximized the Q-value at each step. With A3C, as with the rest of actor-critic methods, we learn two different functions: the policy (or “actor”), and the value (the “critic”). The policy adjusts action probabilities based on the currently estimated advantage of taking that action, and the value function updates that advantage based on the experience and rewards collected by following the policy

The real contribution of A3C comes from its parallelized and asynchronous architecture: multiple actor-learners are dispatched to separate instantiations of the environment; they all interact with the environment and collect experience, and asynchronously push their gradient updates to a central “target network” (an idea borrowed from DQN). Later, OpenAI showed with A2C that asynchronicity does not actually contribute to performance, and in fact reduces sample efficiency.

A3C/A2C can be powerful baseline agents, but they tend to suffer when faced with more complex tasks, severe partial observability, and/or long delays between actions and relevant reward signals. As a result, entire sub-fields of RL research have emerged to address these issues.

The PPO algorithm was introduced by the OpenAI team in 2017 and quickly became one of the most popular RL methods usurping the Deep-Q learning method. It involves collecting a small batch of experiences interacting with the environment and using that batch to update its decision-making policy. Once the policy is updated with this batch, the experiences are thrown away and a newer batch is collected with the newly updated policy. This is the reason why it is an “on-policy learning” approach where the experience samples collected are only useful for updating the current policy once.

The key contribution of PPO is ensuring that a new update of the policy does not change it too much from the previous policy. This leads to less variance in training at the cost of some bias, but ensures smoother training and also makes sure the agent does not go down an unrecoverable path of taking senseless actions.

RL suffers from a problem: Training data is directly dependent on the policy since it is a policy that is responsible for taking actions that lead to observations which are then taken as new data. Data distribution over observation and rewards is always changing. This also makes it susceptible to difficulty in hyperparameter tuning. PPO offers ease of implementation, sample efficiency, ease of tuning. PPO is an on-line policy learning algorithm, it does not use a replay buffer, it, in fact,  learns directly from the agent’s actions on the environment.

Policy gradient methods are less sample efficient than Q-learning since you only use data once after which it is discarded.

The way reinforcement learning models the problem requires several conditions:

You can quantify all the variables the environment describes and have access to these variables at each time step, or state. Neither may be the case in the real world; more often than not you only have access to partial information. The information that you do have access to itself can be inaccurate and in need of further extrapolation since it is measured from an egocentric point of view (at least in the case of a robot interacting with an unknown environment).

You can define a concrete reward function and compute the reward for taking an action. The reward function may not be obvious. For example, if I am designing an agent to perform path planning for an autonomous vehicle, how should we express the reward mathematically? How do we know that the reward function that we defined is "good"? One approach to get around this is inverse reinforcement learning.

You can afford to make mistakes. The freedom to explore without consequence is not always present. If I want to build an autonomous vehicle using RL, how many thousands of times will the car crash itself before it can make even the simplest maneuvers?

Still, training in simulated environments has yielded performance gains in the real world, and should not be dismissed.

Since learning is predominantly online, you have to run trials many many times in order to produce an effective model. This is acceptable when the task at hand is simple, actions are discrete, and information is readily available. But in many cases, the problem formulation is significantly more complex and you must balance the precision of your simulator with both training time and real-time performance constraints.

It is because of these limitations that recent successes in reinforcement learning have happened almost entirely in simulated, controlled environments (think DeepMind's research on Atari, AlphaGo). There is still tremendous research needed in overcoming these limitations and adapting deep RL to work effectively in real-time agents.

Reinforcement learning needs a ton of data or epochs. This is equivalent to thousands of computing hours in a simulator. Such a long time is necessary to learn what humans can usually grasp in a few hours.

For example, Rainbow DQN plays a number of games with the same engine and picks the best algorithm as a comparison. Such an algorithm requires 44 million frames to learn to play with superhuman capabilities. RainbowDQN passes the 100 percent threshold (just above human capabilities) at about 18 million frames. In other words, this is about 83 hours of the play experience. To this number, one should add the time for training the model.

That’s a lot of time! Especially when one considers that Atari games can be picked by a teenager within a few minutes.

In the most common version of reinforcement learning, action sets are discrete. In many realistic use cases, agents perform actions in a continuous space. Making a discrete action, continuous is not only non-trivial but will also increase the number of (discrete) actions the agent will have to deal with during policy optimization. This, in turn, affects training time and performance.

Local optima are harder than those found in deep learning: If one thinks that it can be hard for Stochastic Gradient Descent to get out of local optima, then she should think twice. The level of complexity reached by the optimization procedure of reinforcement learning is much higher than deep learning. It is much more difficult for a reinforcement learning agent to escape local optima.

This is due to the design of the reward function and to the state-action estimator itself (which is a deep neural network).

\section{Future Work}

\subsection{\textbf{Curiosity based learning}}

In many real-world scenarios, rewards extrinsic to the agent are extremely sparse or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent.

Three broad settings are currently being investigated, to make our agent curious:

\begin{itemize}
\item sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal
\item exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently
\item generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch
\end{itemize}

\subsection{\textbf{Distributed Tensorflow}}

Using distributed training, we can train very large models and speed up training time. Tensorflow provides a high-end API to train your models in a distributed way with minimal code changes.  Two types of paradigms used for distributed training:
\begin{itemize}
\item Data parallelism: In data parallelism, models are replicated into different devices (GPU) and trained on batches of data.
\item Model parallelism: When models are too large to fit on a single device then they can be distributed over many devices.
\end{itemize}
We will be investigating the utilization of distributed TensorFlow so that we can train our agents faster and tighten the developer feedback loop.

Most of the competitors and the winners of the Obstacle Tower Challenge have trained for significantly longer timesteps to obtain optimal results for the agent. In the remaining half of the semester, our goal is to reach 5 million timesteps for training.

\subsection{\textbf{IMPALA}}

IMPALA uses an actor-critic set up to learn a policy and a baseline function. The process of generating experiences is decoupled from learning the parameters. The architecture consists of a set of actors, repeatedly generating trajectories of experience and one or more learners that use the experiences sent from actors to learn an off-policy.

At the beginning of each trajectory, an actor updates its own local policy to the latest learner policy and runs it for n steps in its environment. After n steps, the actor sends the trajectory of states, actions, and rewards x1, a1, r1, . . . , xn, an, rn together with the corresponding policy distributions and initial LSTM state to the learner through a queue. The learner then continuously updates its policy on batches of trajectories, each collected from many actors. This simple architecture enables the learner(s) to be accelerated using GPUs and actors to be easily distributed across many machines. However, the learner policy is potentially several updates ahead of the actor’s policy at the time of update, therefore there is a policy-lag between the actors and learner(s). V-trace corrects for this lag to achieve extremely high data throughput while maintaining data efficiency. Using an actor-learner architecture provides fault tolerance like distributed A3C but often has lower communication overhead since the actors send observations rather than parameters/gradients.

With the introduction of very deep model architectures, the speed of a single GPU is often the limiting factor during training. IMPALA can be used with a distributed set of learners to train large neural networks efficiently. Parameters are distributed across the learners and actors retrieve the parameters from all the learners in parallel while only sending observations to a single learner. IMPALA use synchronized parameter update which is vital to maintain data efficiency when scaling to many machines.

\section{Conclusions}

As is evident from the discussion, our agent currently is able to take actions that allow it to progress in the game. However, these are still early days, and with the introduction of distributed training, IMPALA, Curiosity-based learning, etc, our agent will be able to take many optimal decisions and we are expecting it to go past several levels in the Obstacle Tower.

\section{Acknowledgement}

We would like to thank Professor Dr. Michael Zyda and the TA's for allowing us to work on this project.

\begin{thebibliography}{00}
\bibitem{b1} Asynchronous Methods for Deep Reinforcement Learning - Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu (https://arxiv.org/abs/1602.01783)
\bibitem{b2} Human-level control through deep reinforcement learning - Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare (https://www.nature.com/articles/nature14236)
\bibitem{b3} IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures - Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu (https://arxiv.org/abs/1802.01561)
\bibitem{b4} Proximal Policy Optimization Algorithms - John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov(https://arxiv.org/abs/1707.06347)
\bibitem{b5} Planning to Explore via self-supervised World Models - Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, Deepak Pathak (https://arxiv.org/abs/2005.05960)
\bibitem{b6} Sample Efficient Actor-Critic with Experience Replay - Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, Nando de Freitas (https://arxiv.org/abs/1611.01224)
\bibitem{b7} Trust Region Policy Optimization - John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel (https://arxiv.org/abs/1502.05477)
\end{thebibliography}
\vspace{12pt}

\end{document}